{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract metadata\n",
    "\n",
    "This notebook contains scripts that serve to exctract metadata for Jirásek and Světlá from the files as extracted from the digital collections of NKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRASEK_METADATA_PATH = 'C:/Users/valek/Documents/AA-ROZRADIT AZ BUDE NOVY NTB/DH NKP/DateGuesserNKPGit/DateGuesser/Dataset/vystup-jirasek.txt'\n",
    "SVETLA_METADATA_PATH = 'C:/Users/valek/Documents/AA-ROZRADIT AZ BUDE NOVY NTB/DH NKP/DateGuesserNKPGit/DateGuesser/Dataset/vystup-svetla.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'defb0650-affe-11dd-8701-000d606f5dc6': '1874', 'a30280e0-d09f-11dc-a155-000d606f5dc6': '1875', '512c4270-055f-11dd-9584-000d606f5dc6': '1878', '54dc66d0-b4ed-11e9-9209-005056827e51': '1885', '8a2fdf10-f672-11dc-bbc2-000d606f5dc6': '1879', 'bc60e780-be71-11e9-8fdf-005056827e52': '1880', '74192290-bf1a-11e9-8fdf-005056827e52': '1881', '28d2c9e0-b51e-11e9-8fdf-005056827e52': '1881', 'c5602930-e373-11dc-a899-000d606f5dc6': '1882', 'dff52ca0-b50d-11e9-8fdf-005056827e52': '1883', '97ea20c0-8b0e-11de-8062-000d606f5dc6': '1890', '08df98d0-b6ff-11dd-b835-000d606f5dc6': '1883', 'cd18d6b0-12c4-11dd-ab8f-000d606f5dc6': '1883', 'ba63f110-094a-11e5-ae7e-001018b5eb5c': '1885', '7c8272c0-0947-11e5-ae7e-001018b5eb5c': '1889', 'e7551880-1a81-11dd-9082-000d606f5dc6': '1885', 'c7a90bd0-fdb7-11dd-9972-000d606f5dc6': '1885', '59dd5b80-bf45-11dc-8930-000d606f5dc6': '1886', 'b603f190-e8f9-11e4-9c07-001018b5eb5c': '1886', 'cc654660-0d42-11e5-b0b8-5ef3fc9ae867': '1887', '2b411440-a857-11de-b12e-000d606f5dc6': '1887', '99063ab0-f66e-11dc-a1ec-000d606f5dc6': '1888', 'db77b6f0-b6d8-11dd-8fbc-000d606f5dc6': '1888', '24dc0180-01b5-11de-8d54-000d606f5dc6': '1890', '4e4fb6c0-ba06-11dd-8cd4-000d606f5dc6': '1890', '3c5b2730-b7c5-11dd-96e9-000d606f5dc6': '1895', '457424b0-b7c6-11dd-a638-000d606f5dc6': '1898', '11bb9630-d67a-11e7-9c45-005056827e52': '1901', 'afe68830-11c2-11ea-af21-005056827e52': '1907', 'c9de5370-d62f-11dc-acb7-000d606f5dc6': '1896', 'a141f1a0-d630-11dc-895d-000d606f5dc6': '1896', 'cf875600-f2ad-11dd-9680-000d606f5dc6': '1897', '005a1410-9020-11ed-b7c5-5ef3fc9bb22f': '1891', 'b6ec5470-55d5-11de-bc6b-000d606f5dc6': '1891', '271a8310-5668-11de-bb8f-000d606f5dc6': '1892', '6f7f0b10-ef70-11e4-a511-5ef3fc9ae867': '1894', 'd6f02dc0-894b-11dd-ba4a-000d606f5dc6': '1894', '70d31c30-0a5e-11dd-aa2e-000d606f5dc6': '1896', '0c55b3e0-00d7-11e5-93b2-001018b5eb5c': '1897', '527d93b0-d943-11dc-81c2-000d606f5dc6': '1899', '33a057a0-0a5f-11dd-91d7-000d606f5dc6': '1897', 'b4210300-4e73-11eb-b4d1-005056827e51': '1902', '4fd57e30-a2c7-11e3-b833-005056827e52': '1905', 'af1bfa80-283d-11e4-8e0d-005056827e51': '1914', 'a3e9c2d0-5b97-11ef-9a22-5ef3fc9bb22f': '1918', 'fd8b34d0-e6c3-11e8-8d10-5ef3fc9ae867': '1905', 'abead110-7069-11e8-87bd-005056827e52': '1911', '9043fa70-382f-11e4-8e0d-005056827e51': '1913', 'c8283c20-355f-11ef-9fa1-5ef3fc9bb22f': '1913', '2c8396f0-47cc-11e5-a525-5ef3fc9ae867': '1915', '2c518070-0385-11e4-89c6-005056827e51': '1921', 'c00e9800-733d-11e6-81ec-005056827e51': '1921', '015b87b0-0ed8-11e7-968f-005056827e51': '1930', '392d9780-323a-11e6-ae84-005056827e51': '1930', '24a2aec0-7e46-11e5-ac67-005056827e51': '1922'}\n"
     ]
    }
   ],
   "source": [
    "with open(JIRASEK_METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    jirasek_metadata_lines = f.read().split('\\n')\n",
    "\n",
    "jirasek_metadata = {}\n",
    "\n",
    "for line in jirasek_metadata_lines:\n",
    "    if 'UUID: ' in line:\n",
    "        current_uuid = line.split('UUID: ')[1]\n",
    "    \n",
    "    elif 'year: ' in line:\n",
    "        jirasek_metadata[current_uuid] = line.split('year: ')[1]\n",
    "        current_uuid = None\n",
    "\n",
    "print(jirasek_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dfd67d00-d888-11dc-bb94-000d606f5dc6': '1861', '8ccc7b40-e450-11dc-b48e-000d606f5dc6': '1862', '1baf33b0-14d3-11e5-9192-001018b5eb5c': '1865', 'a0a707f0-cfdf-11dc-af3e-000d606f5dc6': '1866', '9c1005a0-9e45-11dc-abe5-000d606f5dc6': '1869', 'ec38f6b0-320e-11dd-b356-000d606f5dc6': '1872', '8ec9c5d0-0555-11dd-8b32-000d606f5dc6': '1872', 'b1587470-393f-11dd-9696-000d606f5dc6': '1872', 'f837aec0-9938-11dd-b3b4-000d606f5dc6': '1874', '621b80d0-b50e-11e9-8fdf-005056827e52': '1880', 'acb14460-6ae0-11dd-96fe-000d606f5dc6': '1882', 'df270000-7b4f-11eb-9d4f-005056827e52': '1886', 'aa871ec0-094a-11e5-ae7e-001018b5eb5c': '1887', 'c96e6410-f66f-11dc-b23a-000d606f5dc6': '1888', '1aa0e260-ae1d-11ee-a51e-005056827e52': '1889', '01b118f0-0eae-11e5-b269-5ef3fc9bb22f': '1897'}\n"
     ]
    }
   ],
   "source": [
    "with open(SVETLA_METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    svetla_metadata_lines = f.read().split('\\n')\n",
    "\n",
    "svetla_metadata = {}\n",
    "\n",
    "for line in svetla_metadata_lines:\n",
    "    if 'UUID: ' in line:\n",
    "        current_uuid = line.split('UUID: ')[1]\n",
    "    \n",
    "    elif 'year: ' in line:\n",
    "        svetla_metadata[current_uuid] = line.split('year: ')[1]\n",
    "        current_uuid = None\n",
    "\n",
    "print(svetla_metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT files\n",
    "\n",
    "The following script serves to extract files according to their uuids from the enriched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENRICHED_FILES_PATH = 'C:/Users/valek/Documents/AA-ROZRADIT AZ BUDE NOVY NTB/DH NKP/New Data for AuthorGuesser/XML_UDPipe'\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "DATA_PATH_JIRASEK = os.path.join(ROOT_PATH, 'Dataset', 'Jirasek_xml')\n",
    "DATA_PATH_SVETLA = os.path.join(ROOT_PATH, 'Dataset', 'Svetla_xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uuid in jirasek_metadata:\n",
    "    if f'{uuid}.xml' in os.listdir(ENRICHED_FILES_PATH):\n",
    "        shutil.copy(os.path.join(ENRICHED_FILES_PATH, f'{uuid}.xml'), os.path.join(DATA_PATH_JIRASEK, f'{uuid}.xml'))\n",
    "    else:\n",
    "        print(f'{uuid} not found in Jirasek data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uuid in svetla_metadata:\n",
    "    if f'{uuid}.xml' in os.listdir(ENRICHED_FILES_PATH):\n",
    "        shutil.copy(os.path.join(ENRICHED_FILES_PATH, f'{uuid}.xml'), os.path.join(DATA_PATH_SVETLA, f'{uuid}.xml'))\n",
    "    else:\n",
    "        print(f'{uuid} not found in Svetla data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delexicalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "autosemantic_pos = ['NOUN', 'ADJ', 'VERB', 'ADV', 'NUM', 'PROPN']\n",
    "\n",
    "# def delexicalise_token(line:str):\n",
    "#     line = line.replace('<w ', '').replace('</w>', '')\n",
    "#     word = line.split('>')[-1]\n",
    "#     line = line.replace(f'>{word}', '')\n",
    "    \n",
    "#     line_elements = line.split(' ')\n",
    "#     attributes = {}\n",
    "#     for element in line_elements:\n",
    "#         attributes.update(extract_attribute(element))\n",
    "\n",
    "#     if attributes['pos'] in autosemantic_pos:\n",
    "#         return f'\\t<token>POS_{attributes[\"pos\"]}</token>'\n",
    "#     else:\n",
    "#         return f'\\t<token>{attributes[\"lemma\"]}</token>'\n",
    "\n",
    "def delexicalise_token(line:str):\n",
    "    while '\\t' in line:\n",
    "        line = line.replace('\\t', '')\n",
    "    pattern = r'<(w|pc)\\s+([^>]+)>([^<]+)</\\1>'\n",
    "    match = re.match(pattern, line)\n",
    "    \n",
    "    if not match:\n",
    "        return line\n",
    "        \n",
    "    tag_type, attributes_str, word = match.groups()\n",
    "    \n",
    "    attr_pattern = r'(\\w+)=\"([^\"]*)\"'\n",
    "    attributes = dict(re.findall(attr_pattern, attributes_str))\n",
    "    \n",
    "    if tag_type == 'w':\n",
    "        if attributes.get('pos') in autosemantic_pos:\n",
    "            return f'\\t<token>POS_{attributes[\"pos\"]}</token>'\n",
    "        else:\n",
    "            return f'\\t<token>{attributes[\"lemma\"]}</token>'\n",
    "    else:  # pc tag\n",
    "        return f'\\t<token>{attributes[\"lemma\"]}</token>'\n",
    "    \n",
    "def delexicalise_file(file_path:str, out_path:str):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    \n",
    "    delexicalised_lines = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            delexicalised_lines.append(delexicalise_token(line))\n",
    "        except:\n",
    "            delexicalised_lines.append('\\t<token>SYM?</token>')\n",
    "            # print(line)\n",
    "    \n",
    "    delex_data = '\\n'.join(delexicalised_lines)\n",
    "\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(delex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRASEK_FINAL_PATH = os.path.join(ROOT_PATH, 'Dataset', 'Jirasek_final')\n",
    "SVETLA_FINAL_PATH = os.path.join(ROOT_PATH, 'Dataset', 'Svetla_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:11<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for file_ in tqdm(os.listdir(DATA_PATH_SVETLA), total=len(os.listdir(DATA_PATH_SVETLA))):\n",
    "    delexicalise_file(os.path.join(DATA_PATH_SVETLA, file_), os.path.join(SVETLA_FINAL_PATH, file_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repair errors\n",
    "\n",
    "the ends of sentences tags have been wrongly placed. The following functions repair them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file in os.listdir(SVETLA_FINAL_PATH):\n",
    "    with open(os.path.join(SVETLA_FINAL_PATH, xml_file), 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    while '</s>' in data:\n",
    "        data = data.replace('</s>', '')\n",
    "    \n",
    "    data_lines = data.split('\\n')\n",
    "\n",
    "    new_lines = []\n",
    "\n",
    "    for i, line in enumerate(data_lines):\n",
    "        if '<s>' in line and i != 0:\n",
    "            new_lines.append('<s>\\n</s>')\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    new_data = '\\n'.join(new_lines)\n",
    "    with open(os.path.join(SVETLA_FINAL_PATH, xml_file), 'w', encoding='utf-8') as f:\n",
    "        f.write(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
